"""Tests for PR comment functionality."""

import json

# Import from e2e helper scripts
import sys
import tempfile
from pathlib import Path
from unittest.mock import mock_open, patch

import pytest

sys.path.append(str(Path(__file__).parent.parent.parent / "helpers"))

from pr_comment import (
    find_latest_report,
    generate_markdown_comment,
    main,
    post_to_github,
)


class TestPRCommentHelper:
    """Test PR comment generation functions."""

    def test_find_latest_report_no_artifacts_dir(self):
        """Test finding latest report when artifacts directory doesn't exist."""
        with tempfile.TemporaryDirectory() as temp_dir:
            project_root = Path(temp_dir)

            with patch("pr_comment.Path") as mock_path:
                mock_path.return_value.parent.parent.parent = project_root
                mock_path.__file__ = __file__

                report = find_latest_report()

                assert report is None

    def test_find_latest_report_empty_artifacts_dir(self):
        """Test finding latest report in empty artifacts directory."""
        with tempfile.TemporaryDirectory() as temp_dir:
            project_root = Path(temp_dir)
            artifacts_dir = project_root / "artifacts"
            artifacts_dir.mkdir()

            with patch("pr_comment.Path") as mock_path:
                mock_path.return_value.parent.parent.parent = project_root
                mock_path.__file__ = __file__

                report = find_latest_report()

                assert report is None

    def test_find_latest_report_multiple_reports(self):
        """Test finding latest report among multiple files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            project_root = Path(temp_dir)
            artifacts_dir = project_root / "artifacts"

            # Create multiple run directories
            run1_dir = artifacts_dir / "run1"
            run2_dir = artifacts_dir / "run2"
            run1_dir.mkdir(parents=True)
            run2_dir.mkdir(parents=True)

            # Create reports with different timestamps
            report1_path = run1_dir / "report.json"
            report2_path = run2_dir / "report.json"
            report1_path.touch()

            # Ensure second report is newer
            import time

            time.sleep(0.01)
            report2_path.touch()

            with patch("pr_comment.Path") as mock_path:
                mock_path.return_value.parent.parent.parent = project_root
                mock_path.__file__ = __file__

                report = find_latest_report()

                assert report == report2_path

    def test_generate_markdown_comment_basic(self):
        """Test generating basic markdown comment."""
        report_data = {
            "results": [
                {
                    "model_name": "resnet50",
                    "device": "CPU",
                    "throughput": 25.5,
                    "latency_avg": 39.2,
                    "threads": 4,
                    "nireq": 1,
                }
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Check basic structure
            assert "## ðŸš€ OVMobileBench E2E Test Results" in comment
            assert "**Android API Level:** 30" in comment
            assert "**Status:** âœ… Passed" in comment

            # Check performance metrics table
            assert "### Performance Metrics" in comment
            assert "| Model | Device | Throughput (FPS) | Latency (ms) | Configuration |" in comment
            assert "|-------|--------|------------------|--------------|---------------|" in comment

            # Check data row
            assert "| resnet50 | CPU | 25.50 | 39.20 | 4 threads, 1 req |" in comment

            # Check best performance
            assert "**Best Performance:** 25.50 FPS" in comment

            # Check footer
            assert "*Generated by OVMobileBench E2E Test*" in comment

        finally:
            report_path.unlink()

    def test_generate_markdown_comment_multiple_results(self):
        """Test generating comment with multiple results."""
        report_data = {
            "results": [
                {
                    "model_name": "resnet50",
                    "device": "CPU",
                    "throughput": 25.5,
                    "latency_avg": 39.2,
                    "threads": 4,
                    "nireq": 1,
                },
                {
                    "model_name": "mobilenet",
                    "device": "GPU",
                    "throughput": 45.8,
                    "latency_avg": 21.8,
                    "threads": 2,
                    "nireq": 2,
                },
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=34)

            # Check API level
            assert "**Android API Level:** 34" in comment

            # Check both models are included
            assert "| resnet50 | CPU | 25.50 | 39.20 | 4 threads, 1 req |" in comment
            assert "| mobilenet | GPU | 45.80 | 21.80 | 2 threads, 2 req |" in comment

            # Check best performance (should be mobilenet at 45.8)
            assert "**Best Performance:** 45.80 FPS" in comment

        finally:
            report_path.unlink()

    def test_generate_markdown_comment_missing_fields(self):
        """Test generating comment with missing optional fields."""
        report_data = {
            "results": [
                {
                    "model_name": "resnet50",
                    "throughput": 25.5,
                    "latency_avg": 39.2,
                    # Missing device, threads, nireq
                }
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Check that N/A values are used for missing fields
            assert "| resnet50 | N/A | 25.50 | 39.20 | N/A threads, N/A req |" in comment

        finally:
            report_path.unlink()

    def test_generate_markdown_comment_empty_results(self):
        """Test generating comment with empty results."""
        report_data = {"results": []}

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Should still have basic structure but no performance table
            assert "## ðŸš€ OVMobileBench E2E Test Results" in comment
            assert "**Android API Level:** 30" in comment
            assert "**Status:** âœ… Passed" in comment

            # Should not have performance metrics section
            assert "### Performance Metrics" not in comment
            assert "**Best Performance:**" not in comment

        finally:
            report_path.unlink()

    def test_generate_markdown_comment_no_results_field(self):
        """Test generating comment without results field."""
        report_data = {"metadata": {"run_id": "test"}}

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Should have basic structure but no performance metrics
            assert "## ðŸš€ OVMobileBench E2E Test Results" in comment
            assert "### Performance Metrics" not in comment

        finally:
            report_path.unlink()

    def test_generate_markdown_comment_zero_throughput(self):
        """Test generating comment with zero throughput values."""
        report_data = {
            "results": [
                {
                    "model_name": "test_model",
                    "device": "CPU",
                    "throughput": 0,
                    "latency_avg": 39.2,
                    "threads": 4,
                    "nireq": 1,
                }
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Should handle zero throughput gracefully
            assert "| test_model | CPU | 0.00 | 39.20 | 4 threads, 1 req |" in comment

            # Should not show best performance for zero throughput
            assert "**Best Performance:** 0.00 FPS" in comment

        finally:
            report_path.unlink()

    def test_post_to_github_output_and_file(self):
        """Test posting comment to GitHub (mock implementation)."""
        test_comment = "Test comment content"
        pr_number = 123

        with patch("builtins.print") as mock_print:
            with patch("builtins.open", mock_open()) as mock_file:
                post_to_github(test_comment, pr_number)

                # Check that comment was printed
                mock_print.assert_called_once_with(test_comment)

                # Check that file was created
                mock_file.assert_called_once_with("/tmp/pr_comment.md", "w")
                mock_file.return_value.write.assert_called_once_with(test_comment)

    def test_post_to_github_file_write_error(self):
        """Test posting comment when file write fails."""
        test_comment = "Test comment content"
        pr_number = 123

        with patch("builtins.print"):
            with patch("builtins.open", side_effect=IOError("Write error")):
                with pytest.raises(IOError, match="Write error"):
                    post_to_github(test_comment, pr_number)


class TestPRCommentMain:
    """Test PR comment main function."""

    def test_main_no_report_found(self):
        """Test main function when no report is found."""
        with patch("pr_comment.find_latest_report", return_value=None):
            with patch("sys.argv", ["pr_comment.py", "--api", "30"]):
                main()  # Should not raise exception

    def test_main_with_pr_number(self):
        """Test main function with PR number specified."""
        mock_report_path = Path("test_report.json")
        test_comment = "Generated comment"

        with patch("pr_comment.find_latest_report", return_value=mock_report_path):
            with patch("pr_comment.generate_markdown_comment", return_value=test_comment):
                with patch("pr_comment.post_to_github") as mock_post:
                    with patch("sys.argv", ["pr_comment.py", "--api", "30", "--pr", "123"]):
                        main()

                        mock_post.assert_called_once_with(test_comment, 123)

    def test_main_without_pr_number(self):
        """Test main function without PR number (print mode)."""
        mock_report_path = Path("test_report.json")
        test_comment = "Generated comment"

        with patch("pr_comment.find_latest_report", return_value=mock_report_path):
            with patch("pr_comment.generate_markdown_comment", return_value=test_comment):
                with patch("builtins.print") as mock_print:
                    with patch("sys.argv", ["pr_comment.py", "--api", "30"]):
                        main()

                        mock_print.assert_called_with(test_comment)

    def test_main_missing_api_argument(self):
        """Test main function without required API argument."""
        with patch("sys.argv", ["pr_comment.py"]):
            with pytest.raises(SystemExit):  # argparse should exit
                main()

    def test_main_comment_generation_error(self):
        """Test main function when comment generation fails."""
        mock_report_path = Path("test_report.json")

        with patch("pr_comment.find_latest_report", return_value=mock_report_path):
            with patch(
                "pr_comment.generate_markdown_comment",
                side_effect=Exception("Generation error"),
            ):
                with patch("sys.argv", ["pr_comment.py", "--api", "30"]):
                    with pytest.raises(Exception, match="Generation error"):
                        main()


class TestPRCommentIntegration:
    """Integration tests for PR comment functionality."""

    def test_complete_pr_comment_workflow(self):
        """Test complete workflow from finding report to generating comment."""
        with tempfile.TemporaryDirectory() as temp_dir:
            project_root = Path(temp_dir)
            artifacts_dir = project_root / "artifacts"
            run_dir = artifacts_dir / "test_run"
            run_dir.mkdir(parents=True)

            # Create comprehensive report
            report_data = {
                "results": [
                    {
                        "model_name": "resnet50",
                        "device": "CPU",
                        "throughput": 25.5,
                        "latency_avg": 39.2,
                        "threads": 4,
                        "nireq": 1,
                    },
                    {
                        "model_name": "mobilenet",
                        "device": "CPU",
                        "throughput": 45.8,
                        "latency_avg": 21.8,
                        "threads": 4,
                        "nireq": 1,
                    },
                ]
            }

            report_path = run_dir / "report.json"
            with open(report_path, "w") as f:
                json.dump(report_data, f)

            with patch("pr_comment.Path") as mock_path:
                mock_path.return_value.parent.parent.parent = project_root
                mock_path.__file__ = __file__

                # Find latest report
                latest_report = find_latest_report()
                assert latest_report == report_path

                # Generate comment
                comment = generate_markdown_comment(latest_report, api_level=30)

                # Verify comment structure
                assert "## ðŸš€ OVMobileBench E2E Test Results" in comment
                assert "resnet50" in comment
                assert "mobilenet" in comment
                assert "**Best Performance:** 45.80 FPS" in comment

    def test_pr_comment_markdown_formatting(self):
        """Test that generated markdown is properly formatted."""
        report_data = {
            "results": [
                {
                    "model_name": "test_model",
                    "device": "CPU",
                    "throughput": 123.456,
                    "latency_avg": 78.901,
                    "threads": 8,
                    "nireq": 4,
                }
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=34)

            # Verify markdown table formatting
            lines = comment.split("\n")

            # Find table lines
            header_line = None
            separator_line = None
            data_line = None

            for i, line in enumerate(lines):
                if "| Model | Device |" in line:
                    header_line = line
                    if i + 1 < len(lines):
                        separator_line = lines[i + 1]
                    if i + 2 < len(lines):
                        data_line = lines[i + 2]
                    break

            # Verify table structure
            assert header_line is not None
            assert separator_line is not None
            assert data_line is not None

            # Check separator line format
            assert separator_line.startswith("|")
            assert separator_line.endswith("|")
            assert "-------" in separator_line

            # Check data formatting (numbers should be formatted to 2 decimal places)
            assert "123.46" in data_line
            assert "78.90" in data_line

        finally:
            report_path.unlink()

    def test_pr_comment_special_characters(self):
        """Test PR comment generation with special characters in model names."""
        report_data = {
            "results": [
                {
                    "model_name": "model-with-dashes_and_underscores",
                    "device": "CPU/GPU",
                    "throughput": 25.5,
                    "latency_avg": 39.2,
                    "threads": 4,
                    "nireq": 1,
                }
            ]
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(report_data, f)
            report_path = Path(f.name)

        try:
            comment = generate_markdown_comment(report_path, api_level=30)

            # Should handle special characters in model names
            assert "model-with-dashes_and_underscores" in comment
            assert "CPU/GPU" in comment

            # Should not break markdown formatting
            assert "|" in comment
            assert "**" in comment

        finally:
            report_path.unlink()
